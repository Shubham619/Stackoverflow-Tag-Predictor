{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f85e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "#Purpose: Computes the overall distribution of servers with DRAM errors and failures over time.\n",
    "def overall_distribution(df_log, df_tickets):\n",
    "    total_server = 258993 ## total distinc num of servers\n",
    "    monthly_server_population = [185961,190428,196497,198658,202245,205709,213998,236471]\n",
    "    monthly_server_with_error = []\n",
    "    monthly_server_with_failure = []\n",
    "    counter = 0\n",
    "    for i in ['0001-01','0001-02','0001-03','0001-04','0001-05','0001-06','0001-07','0001-08']:\n",
    "        frac_error_server = len(df_log[df_log['month'] == i].sid.unique()) / monthly_server_population[counter]\n",
    "        monthly_server_with_error.append(frac_error_server)\n",
    "        frac_failed_server = len(df_tickets[df_tickets['month'] == i].sid.unique()) / monthly_server_population[counter]\n",
    "        monthly_server_with_failure.append(frac_failed_server)\n",
    "        counter = counter + 1\n",
    "    ff = open('./result/overall_distribution.txt',\"w\")\n",
    "    print(\"Monthly fraction of server with errors: mean \",np.mean(monthly_server_with_error), file=ff)\n",
    "    print(monthly_server_with_error,file=ff)\n",
    "    print(\"Fraction of server with errorr in eight month \",len(df_log.sid.unique()) / total_server, file=ff)\n",
    "    print(\"Monthly fraction of server with failures: mean\",np.mean(monthly_server_with_failure), file=ff)\n",
    "    print(monthly_server_with_failure,file=ff)\n",
    "    print(\"Overall error rate in eight month \",len(df_log.sid.unique()) / total_server)\n",
    "    print(\"Average error rate: \", np.mean(monthly_server_with_error))\n",
    "    print(\"Average failure rate: \", np.mean(monthly_server_with_failure))\n",
    "    print(\"Overall distribution analysis done!\")\n",
    "    ff.close()\n",
    "#computes the overall error rate for the given months.\n",
    "\n",
    "#Calculates the time difference between when an error occurred and when a failure was logged\n",
    "def compute_time_diff(df_tickets_log):\n",
    "    base ={'01':0,'02':31,'03':61,'04':92,'05':123,'06':152,'07':183,'08':213}\n",
    "    df_tickets_log['error_date'] = df_tickets_log['error_time'].str[5:7]\n",
    "    df_tickets_log['error_date'] = df_tickets_log['error_date'].apply(lambda x: base[x])\n",
    "    df_tickets_log['failed_date'] = df_tickets_log['failed_time'].str[5:7]\n",
    "    df_tickets_log['failed_date'] = df_tickets_log['failed_date'].apply(lambda x: base[x])\n",
    "    df_tickets_log['error_time_in_day'] = df_tickets_log['error_time'].str[8:10].astype(int)\n",
    "    df_tickets_log['failed_time_in_day'] = df_tickets_log['failed_time'].str[8:10].astype(int)\n",
    "    df_tickets_log['error_time_offset'] = pd.to_datetime(df_tickets_log['error_time'].str[11:20])\n",
    "    df_tickets_log['failed_time_offset'] = pd.to_datetime(df_tickets_log['failed_time'].str[11:20])\n",
    "    df_tickets_log['time_diff'] = (df_tickets_log['failed_time_offset'] - df_tickets_log['error_time_offset']).dt.total_seconds() + (df_tickets_log['failed_time_in_day'] + df_tickets_log['failed_date'] - df_tickets_log['error_time_in_day'] - df_tickets_log['error_date']) * 24 * 3600\n",
    "#It computes time offsets for error and failure timestamps and derives the time difference in seconds.\n",
    "\n",
    "#Analyzes how predictable server failures are based on DRAM errors within different time windows.\n",
    "def predictable_analysis(df_tickets_log, tickets):   ### Finding 2\n",
    "    num_total = {}\n",
    "    for i in [1,2,3]: ## for each type of tickets\n",
    "        num_total[i] = len(tickets[tickets['failure_type'] == i])\n",
    "    f = open('./result/predictale_analysis.txt','w')\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    print(\"time\\tUE-driven\\tCE-driven\\tMisc\",file=f)\n",
    "    count = 0\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        row_res = str(time_index[count])\n",
    "        df_tm = df_tickets_log[df_tickets_log['time_diff'] > tm.total_seconds()]\n",
    "        for typ in [1,2,3]:\n",
    "            row_res = row_res + '\\t' + \"{:.6f}\".format(len(df_tm[df_tm['failure_type'] == typ].sid.unique()) / num_total[typ])\n",
    "        print(row_res,file=f)\n",
    "        count = count + 1\n",
    "    f.close()\n",
    "    print(\"Predictable analsyis done!\")\n",
    "#Generates a text file with the analysis of server failure predictability based on different time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e58ec07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 00:01:00')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Timedelta(minutes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2471749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall error rate in eight month  0.11774835613317734\n",
      "Average error rate:  0.052337384475264204\n",
      "Average failure rate:  0.052337384475264204\n",
      "Overall distribution analysis done!\n"
     ]
    }
   ],
   "source": [
    "overall_distribution(df_log,df_log)   ## Finding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddbe7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge tickets and mcelog\n",
    "df_tickets_log = df_log[df_log['sid'].isin(df_tickets.sid.unique())]\n",
    "df_tickets_log = df_tickets_log.reset_index(drop=True)\n",
    "tickets = df_tickets[df_tickets['failure_type'] > 0]\n",
    "df_tickets_log = df_tickets_log.merge(tickets.loc[:,['sid','failed_time','failure_type']],how='inner', on='sid')\n",
    "#adds additional columns present in df_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc9f66d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictable analsyis done!\n"
     ]
    }
   ],
   "source": [
    "## compute time diff\n",
    "compute_time_diff(df_tickets_log)\n",
    "df_tickets_log = df_tickets_log.drop(columns={'error_date', 'failed_date','error_time_in_day', 'failed_time_in_day','error_time_offset','failed_time_offset'})\n",
    "\n",
    "predictable_analysis(df_tickets_log, tickets) ## Finding 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2272985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1%12/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e3892fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzes the average number of correctable errors (CE) per failure.\n",
    "def num_ce_analysis(df_tickets_log):   ### Finding 3\n",
    "    f = open('./result/num_ce_analysis.txt','w')\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    print(\"time\\tUE-driven\\tCE-driven\\tMisc\",file=f)\n",
    "    count = 0\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        row_res = str(time_index[count])\n",
    "        df_tm = df_tickets_log[df_tickets_log['time_diff'] > tm.total_seconds()]\n",
    "        for typ in [1,2,3]:\n",
    "            df_typ = df_tm[df_tm['failure_type'] == typ].groupby('sid').error_time.count().reset_index(name='val')\n",
    "            row_res = row_res + '\\t' + \"{:.4f}\".format(df_typ['val'].mean())\n",
    "        print(row_res,file=f)\n",
    "        count = count + 1\n",
    "    f.close()\n",
    "    print(\"Average umber of CE per failure analsyis done!\")\n",
    "# Outputs the average number of CEs per failure for different failure types and time windows.\n",
    "#mean across all server\n",
    "\n",
    "#Computes the Mean Time Between Errors (MTBE).\n",
    "#it means how much time passes between errors for each server\n",
    "def mtbe_analysis(df_tickets_log):   ### Finding 4\n",
    "    f = open('./result/mtbe_analysis.txt','w')\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    print(\"time\\tUE-driven\\tCE-driven\\tMisc\",file=f)\n",
    "    count = 0\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        row_res = str(time_index[count])\n",
    "        df_tm = df_tickets_log[df_tickets_log['time_diff'] > tm.total_seconds()]\n",
    "        for typ in [1,2,3]:\n",
    "            df_typ = df_tm[df_tm['failure_type'] == typ].sort_values(by=['sid','error_time'])\n",
    "            df_typ['res'] = df_typ.groupby('sid')['time_diff'].diff()\n",
    "            df_typ = df_typ.dropna()\n",
    "            df_typ['res'] = -df_typ['res']\n",
    "            df_res = df_typ.groupby('sid')['res'].mean().reset_index(name='MTBE')\n",
    "            row_res = row_res + '\\t' + \"{:.4f}\".format(df_res['MTBE'].median() / 60.0)\n",
    "        print(row_res,file=f)\n",
    "        count = count + 1\n",
    "    f.close()\n",
    "    print(\"Mean time between error analsyis done!\")\n",
    "#Provides an analysis of the mean time between errors \n",
    "\n",
    "#Analyzes DRAM errors at different hardware component levels (socket, channel, bank, etc.).\n",
    "def component_breakdown(df):   ### Preliminary for Finding 5 and 6\n",
    "    warnings.filterwarnings('ignore')\n",
    "    df['socketid'] = df['memoryid'].apply(lambda x: 0 if x < 12 else 1)\n",
    "    df['channelid'] = df['memoryid'].apply(lambda x: int((x % 12) / 2))\n",
    "    df['dimmid'] = df['memoryid'].apply(lambda x : x % 2)\n",
    "    res_ce_num = {}\n",
    "    res_sid_num = {}\n",
    "\n",
    "    ## socket failures\n",
    "    #shows how many errors occurred for each combination of server, socket, and memory channel\n",
    "    #how many different combination of x1,x2,x3 are same and .y are different (counts)\n",
    "    grouped = df.groupby(['sid','socketid','channelid']).error_time.count().reset_index(name='num')\n",
    "    df_channel=grouped.groupby(['sid','socketid']).channelid.count().reset_index(name='channel_count')\n",
    "    df_error=grouped.groupby(['sid','socketid']).num.sum().reset_index(name='error_count')\n",
    "    df_channel_error = pd.merge(df_channel,df_error,how='inner',on=['sid','socketid'])\n",
    "    df_socket_result= df_channel_error[(df_channel_error['error_count'] > 1000) & (df_channel_error['channel_count'] > 1)]\n",
    "    df=pd.merge(df,df_socket_result,how='left',on=['sid','socketid'])\n",
    "    df_res=df[(df['error_count'] > 1000) & (df['channel_count'] > 1)]\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        df_tm = df_res[df_res['time_diff'] > tm.total_seconds()]\n",
    "        res_ce_num[tm] = str(len(df_tm))\n",
    "        res_sid_num[tm] = str(len(df_tm.sid.unique()))\n",
    "    df=df[~((df['error_count'] > 1000) & (df['channel_count'] > 1))].drop(columns=['error_count','channel_count'])\n",
    "\n",
    "    ## channel failures\n",
    "    grouped=df.groupby(['sid','socketid','channelid','dimmid','rankid','bankid']).error_time.count().reset_index(name='num')\n",
    "    df_bank=grouped.groupby(['sid','socketid','channelid','dimmid','rankid']).bankid.count().reset_index(name='bank_count')\n",
    "    df_error=grouped.groupby(['sid','socketid','channelid','dimmid','rankid']).num.sum().reset_index(name='error_count')\n",
    "    df_bank_error = pd.merge(df_bank,df_error,how='inner',on=['sid','socketid','channelid','dimmid','rankid'])\n",
    "    df_error_total=df_bank_error.groupby(['sid','socketid','channelid']).error_count.sum().reset_index(name='total_error_count')\n",
    "    df_bank_total=df_bank_error.groupby(['sid','socketid','channelid']).bank_count.sum().reset_index(name='total_bank_count')\n",
    "    df_total_result=pd.merge(df_error_total,df_bank_total,how='inner',on=['sid','socketid','channelid'])\n",
    "    df_channel_result= df_total_result[(df_total_result['total_error_count'] > 1000) & (df_total_result['total_bank_count'] > 1)]\n",
    "    df=pd.merge(df,df_channel_result,how='left',on=['sid','socketid','channelid'])\n",
    "    df_res=df[(df['total_error_count'] > 1000) & (df['total_bank_count'] > 1)]\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        df_tm = df_res[df_res['time_diff'] > tm.total_seconds()]\n",
    "        res_ce_num[tm] = res_ce_num[tm] + ' ' + str(len(df_tm))\n",
    "        res_sid_num[tm] = res_sid_num[tm]  + ' ' + str(len(df_tm.sid.unique()))\n",
    "    df=df[~((df['total_error_count'] > 1000) & (df['total_bank_count'] > 1))].drop(columns=['total_error_count','total_bank_count'])\n",
    "\n",
    "    ## bank failures\n",
    "    grouped=df.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','row']).error_time.count().reset_index(name='num')\n",
    "    df_row=grouped.groupby(['sid','socketid','channelid','dimmid','rankid','bankid']).row.count().reset_index(name='row_count')\n",
    "    df_error=grouped.groupby(['sid','socketid','channelid','dimmid','rankid','bankid']).num.sum().reset_index(name='error_count')\n",
    "    df_row_error = pd.merge(df_row,df_error,how='inner',on=['sid','socketid','channelid','dimmid','rankid','bankid'])\n",
    "    df_bank_result= df_row_error[(df_row_error['error_count'] > 1000) & (df_row_error['row_count'] > 1)]\n",
    "    df=pd.merge(df,df_bank_result,how='left',on=['sid','socketid','channelid','dimmid','rankid','bankid'])\n",
    "    df_res=df[(df['error_count'] > 1000) & (df['row_count'] > 1)]\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        df_tm = df_res[df_res['time_diff'] > tm.total_seconds()]\n",
    "        res_ce_num[tm] = res_ce_num[tm] + ' ' + str(len(df_tm))\n",
    "        res_sid_num[tm] = res_sid_num[tm]  + ' ' + str(len(df_tm.sid.unique()))\n",
    "    df=df[~((df['error_count'] > 1000) & (df['row_count'] > 1))].drop(columns=['error_count','row_count'])\n",
    "\n",
    "    ## row failures\n",
    "    grouped=df.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','row','col']).error_time.count().reset_index(name='num')\n",
    "    df_column=grouped.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','row']).col.count().reset_index(name='col_count')\n",
    "    df_error=grouped.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','row']).num.sum().reset_index(name='error_count')\n",
    "\n",
    "    df_column_error = pd.merge(df_column,df_error,how='inner',on=['sid','socketid','channelid','dimmid','rankid','bankid','row'])\n",
    "    df_row_result= df_column_error[df_column_error['col_count'] > 1]\n",
    "    df=pd.merge(df,df_row_result,how='left',on=['sid','socketid','channelid','dimmid','rankid','bankid','row'])\n",
    "    df_res=df[df['col_count'] > 1]\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        df_tm = df_res[df_res['time_diff'] > tm.total_seconds()]\n",
    "        res_ce_num[tm] = res_ce_num[tm] + ' ' + str(len(df_tm))\n",
    "        res_sid_num[tm] = res_sid_num[tm]  + ' ' + str(len(df_tm.sid.unique()))\n",
    "    df=df[~(df['col_count'] > 1)].drop(columns=['error_count','col_count'])\n",
    "\n",
    "    ## column failures\n",
    "    grouped=df.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','row','col']).error_time.count().reset_index(name='num')\n",
    "    df_row=grouped.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','col']).row.count().reset_index(name='row_count')\n",
    "    df_error=grouped.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','col']).num.sum().reset_index(name='error_count')\n",
    "    df_row_error = pd.merge(df_row,df_error,how='inner',on=['sid','socketid','channelid','dimmid','rankid','bankid','col'])\n",
    "    df_col_result= df_row_error[df_row_error['row_count'] > 1]\n",
    "    df=pd.merge(df,df_col_result,how='left',on=['sid','socketid','channelid','dimmid','rankid','bankid','col'])\n",
    "    df_res=df[df['row_count'] > 1]\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        df_tm = df_res[df_res['time_diff'] > tm.total_seconds()]\n",
    "        res_ce_num[tm] = res_ce_num[tm] + ' ' + str(len(df_tm))\n",
    "        res_sid_num[tm] = res_sid_num[tm]  + ' ' + str(len(df_tm.sid.unique()))\n",
    "    df=df[~(df['row_count'] > 1)].drop(columns=['error_count','row_count'])\n",
    "\n",
    "    ## cell failures\n",
    "    df=df.sort_values(by=['sid','socketid','channelid','dimmid','rankid','bankid','row','col','error_time'])\n",
    "    df['time_res']=df.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','row','col'])['time_diff'].diff().fillna(-70)\n",
    "    df['time_res']=-df['time_res']\n",
    "    df_cell=df.groupby(['sid','socketid','channelid','dimmid','rankid','bankid','row','col'])['time_res'].min().reset_index(name='min_diff')\n",
    "    df=pd.merge(df,df_cell,how='left',on=['sid','socketid','channelid','dimmid','rankid','bankid','row','col'])\n",
    "    df_res=df[df['min_diff'] <= 60]\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        df_tm = df_res[df_res['time_diff'] > tm.total_seconds()]\n",
    "        res_ce_num[tm] = res_ce_num[tm] + ' ' + str(len(df_tm))\n",
    "        res_sid_num[tm] = res_sid_num[tm]  + ' ' + str(len(df_tm.sid.unique()))\n",
    "\n",
    "    ## random errors failure\n",
    "    df_res=df[df['min_diff'] > 60]\n",
    "    for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        df_tm = df_res[df_res['time_diff'] > tm.total_seconds()]\n",
    "        res_ce_num[tm] = res_ce_num[tm] + ' ' + str(len(df_tm))\n",
    "        res_sid_num[tm] = res_sid_num[tm]  + ' ' + str(len(df_tm.sid.unique()))\n",
    "\n",
    "    return res_ce_num, res_sid_num\n",
    "# Identifies the components most prone to DRAM errors over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df8acda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average umber of CE per failure analsyis done!\n",
      "Mean time between error analsyis done!\n"
     ]
    }
   ],
   "source": [
    "num_ce_analysis(df_tickets_log)  ## Finding 3\n",
    "\n",
    "mtbe_analysis(df_tickets_log) ## Finding 4\n",
    "\n",
    "num_ce, num_res=component_breakdown_main(df_tickets_log)  ## Finding 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0aec6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_breakdown_main(df):\n",
    "    df_res = df.loc[:,['sid','memoryid','rankid','bankid','row','col','error_time','failure_type','time_diff']]\n",
    "    num_ce_res = {}\n",
    "    num_sid_res = {}\n",
    "    for typ in [1,2,3]:\n",
    "        df_type = df_res[df_res['failure_type'] == typ]\n",
    "        ce, sid= component_breakdown(df_type)\n",
    "        num_ce_res[typ] = ce\n",
    "        num_sid_res[typ] = sid\n",
    "    return num_ce_res, num_sid_res\n",
    "\n",
    "def frac_failure_per_component(sid_res, prediction_window, idx):   ## Finding 5 Part 1\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    component_index = ['socket', 'channel','bank','row','column','cell','random']\n",
    "    type_index = [\"ue\",\"ce\",\"misc\"]\n",
    "    f = open(\"./result/frac_failure_per_compoent_\" + str(time_index[idx]) + \".txt\",\"w\")\n",
    "    print(\"component\\tresult\\tfailure_type\",file=f)\n",
    "    ## total number of failures for each failures type and each prediction window from Finding 2\n",
    "    total_num = { 1: [547,483,463,422,393,357,190,133,97],\n",
    "                  2: [792,733,668,532,388,361,257,216,166],\n",
    "                  3: [760,758,752,744,739,723,307,233,172]\n",
    "                }\n",
    "    for typ in [1,2,3]:\n",
    "        lst = sid_res[typ][prediction_window].split()\n",
    "        for i in range(len(lst)):\n",
    "            print(component_index[i]+\"\\t\"+str(int(lst[i])/total_num[typ][idx])+\"\\t\"+type_index[typ-1],file=f)\n",
    "    f.close()\n",
    "    print(\"Fraction of failures per component analysis for window \" + time_index[idx] + \" done!\")\n",
    "\n",
    "def frac_ce_per_component(num_ce, time, idx):  ## Finding 5 Part 2\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    component_index = ['socket', 'channel','bank','row','column','cell','random']\n",
    "    type_index = [\"ue\",\"ce\",\"misc\"]\n",
    "    f = open(\"./result/frac_ce_per_compoent_\" + str(time_index[idx]) + \".txt\",\"w\")\n",
    "    print(\"component\\tvalue\\tfailure_type\",file=f)\n",
    "    for typ in [1,2,3]:\n",
    "        lst = num_ce[typ][time].split()\n",
    "        total_num = 0\n",
    "        for i in range(len(lst)):\n",
    "            total_num = total_num + int(lst[i])\n",
    "        for i in range(len(lst)):\n",
    "            print(component_index[i]+\"\\t\"+str(int(lst[i])/total_num)+\"\\t\"+type_index[typ-1], file=f)\n",
    "    print(\"Fraction of CEs per component analysis for window \" + time_index[idx] + \" done!\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9bdc76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of failures per component analysis for window 5m done!\n",
      "Fraction of CEs per component analysis for window 5m done!\n"
     ]
    }
   ],
   "source": [
    "frac_failure_per_component(num_res,pd.Timedelta(minutes=5),1)\n",
    "frac_ce_per_component(num_ce,pd.Timedelta(minutes=5),1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ad00cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_breakdown_by_component(num_ce,failure_type): ### Finding 6\n",
    "    component_index = [\"socket\",\"channel\",\"bank\",\"row\",\"column\",\"cell\",\"random\"]\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    type_index = [\"ue\",\"ce\",\"misc\"]\n",
    "    total_num = [269733,6897677,4181363]\n",
    "    f = open(\"./result/failures_\" + type_index[failure_type-1] + \"_breakdown.txt\",\"w\")\n",
    "    print(\"time\\tvalue\\tcomponent\",file=f)\n",
    "    counter = 1\n",
    "    for time in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "        lst = num_ce[failure_type][time].split()\n",
    "        ## focus on channel, bank, row, column\n",
    "        for i in [1,2,3,4]:\n",
    "            print(time_index[counter-1]+\"\\t\"+str(int(lst[i])/total_num[failure_type-1])+\"\\t\"+component_index[i], file=f)\n",
    "        counter = counter + 1\n",
    "    f.close()\n",
    "    print('Error breakdown by component analysis for failures type ' + type_index[failure_type-1] + ' done!')\n",
    "\n",
    "def hardware_configuration_impact_analysis(df_res, factor, idx):   ### Finding 7, 8, 9\n",
    "    total_failures = {1:567, 2:809, 3:761} ## overall number of failures of each types\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    type_index = [\"ue\",\"ce\",\"misc\"]\n",
    "    factors = [['A1','A2','B1','B2','B3','C1','C2'],\n",
    "             [8, 12,16,24],\n",
    "             ['M1','M2','M3','M4']]\n",
    "    dimm_num_name = {8:'8-dimm',12:'12-dimm',16:'16-dimm',24:'24-dimm'}\n",
    "    for typ in [1,2,3]:\n",
    "        f = open(\"./result/\" + factor + \"_\" + type_index[typ-1] + \"_breakdown.txt\",\"w\")\n",
    "        print('time\\tfraction\\tconfiguration', file = f)\n",
    "        df_final = df_res[df_res['failure_type'] == typ]\n",
    "        counter = 1\n",
    "        for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "            df_tm = df_final[df_final['time_diff'] > tm.total_seconds()]\n",
    "            for i in factors[idx]:\n",
    "                value = len(df_tm[df_tm[factor] == i].sid.unique()) / total_failures[typ]\n",
    "                if i == 8 or i == 12 or i == 16 or i == 24:\n",
    "                    print(time_index[counter-1] + '\\t' + str(value) + '\\t' + dimm_num_name[i], file = f)\n",
    "                else:\n",
    "                    print(time_index[counter-1] + '\\t' + str(value) + '\\t' + str(i), file = f)\n",
    "            counter = counter + 1\n",
    "        f.close()\n",
    "\n",
    "def failure_rate_breakdown(df_res, factor):   ## Finding 7 additional\n",
    "    model_population = [77670,71641,38006,514,10540,32534,15549]\n",
    "    dimm_population = [35676,151988,7085,51705]\n",
    "    server_population = [58161,127818,52931,7544]\n",
    "    df_final = df_res.drop_duplicates('sid')\n",
    "    res = []\n",
    "    idx = 0;\n",
    "    if factor == \"DRAM_model\":\n",
    "        for mod in ['A1','A2','B1','B2','B3','C1','C2']:\n",
    "            res.append(len(df_final[df_final['DRAM_model'] == mod])/model_population[idx] * 100)\n",
    "            idx = idx + 1\n",
    "    elif factor == 'DIMM_number':\n",
    "        for dimm in [8,12,16,24]:\n",
    "            res.append(len(df_final[df_final['DIMM_number'] == dimm])/ dimm_population[idx] * 100)\n",
    "            idx = idx + 1\n",
    "    elif factor == 'server_manufacturer':\n",
    "        for ser in ['M1','M2','M3','M4']:\n",
    "            res.append(len(df_final[df_final['server_manufacturer'] == ser])/ server_population[idx] * 100)\n",
    "            idx = idx + 1\n",
    "    print(factor + \" failure rate breakdown: \", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c9e1588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAM_model pupulation:  [77670, 71641, 38006, 514, 10540, 32534, 15549]\n",
      "DRAM_model failure number breakdown:  [123, 39, 303, 6, 16, 194, 25]\n",
      "DRAM_model unpredictabble failures:  [8, 3, 25, 1, 1, 14, 2]\n"
     ]
    }
   ],
   "source": [
    "hardware_configuration_impact_analysis(df_tickets_log,'DRAM_model', 0)  ## Finding 6\n",
    "failure_number_breakdown(df_tickets_log,'DRAM_model') ## Finding 6\n",
    "\n",
    "hardware_configuration_impact_analysis(df_tickets_log,'DIMM_number', 1)  ## Finding 7\n",
    "\n",
    "hardware_configuration_impact_analysis(df_tickets_log,'server_manufacturer', 2)  ## Finding 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "105fb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failure_number_breakdown(df_res, factor):   ## Finding 7 additional \n",
    "    model_population = [77670,71641,38006,514,10540,32534,15549]\n",
    "    dimm_population = [35676,151988,7085,51705]\n",
    "    server_population = [58161,127818,52931,7544]\n",
    "    df_final = df_res.drop_duplicates('sid')\n",
    "    predictable_df = df_res[df_res['time_diff'] > pd.Timedelta(minutes=5).total_seconds()]\n",
    "    df_unpredictable = df_final[~df_final['sid'].isin(predictable_df.sid.unique())]\n",
    "    res = []\n",
    "    unpredict_res = []\n",
    "    population_res = []\n",
    "    idx = 0;\n",
    "    if factor == \"DRAM_model\":\n",
    "        population_res = model_population\n",
    "        for mod in ['A1','A2','B1','B2','B3','C1','C2']:\n",
    "            res.append(len(df_final[df_final['DRAM_model'] == mod]))\n",
    "            unpredict_res.append(len(df_unpredictable[df_unpredictable['DRAM_model'] == mod]))\n",
    "            idx = idx + 1\n",
    "    elif factor == 'DIMM_number':\n",
    "        population_res = dimm_population\n",
    "        for dimm in [8,12,16,24]:\n",
    "            res.append(len(df_final[df_final['DIMM_number'] == dimm]))\n",
    "            unpredict_res.append(len(df_unpredictable[df_unpredictable['DIMM_number'] == dimm]))\n",
    "            idx = idx + 1\n",
    "    elif factor == 'server_manufacturer':\n",
    "        population_res = server_population\n",
    "        for ser in ['M1','M2','M3','M4']:\n",
    "            res.append(len(df_final[df_final['server_manufacturer'] == ser]))\n",
    "            unpredict_res.append(len(df_unpredictable[df_unpredictable['server_manufacturer'] == ser]))\n",
    "            idx = idx + 1\n",
    "    print(factor + \" pupulation: \", population_res)\n",
    "    print(factor + \" failure number breakdown: \", res)\n",
    "    print(factor + \" unpredictabble failures: \", unpredict_res)    \n",
    "\n",
    "def read_scrubbing_analysis(df_res):  ### Finding 10\n",
    "    type_index = [\"ue\",\"ce\",\"misc\"]\n",
    "    type_population = {1:547, 2:792, 3:760}\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    f1 = open(\"./result/read_error_mean.txt\",\"w\")\n",
    "    f2 = open(\"./result/scrub_error_mean.txt\",\"w\")\n",
    "    for ff in [f1, f2]:\n",
    "        print('time\\tvalue\\tfailure_type',file=ff)\n",
    "    for typ in [1,2,3]:\n",
    "        counter = 1\n",
    "        df_type = df_res[df_res['failure_type'] == typ]\n",
    "        for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "            df_tm = df_type[df_type['time_diff'] > tm.total_seconds()]\n",
    "            df_read = df_tm[df_tm['error_type'] == 1].groupby('sid')['error_time'].count().reset_index(name='error_num')\n",
    "            df_scrb = df_tm[df_tm['error_type'] == 2].groupby('sid')['error_time'].count().reset_index(name='error_num')\n",
    "            print(time_index[counter-1] + '\\t' + str(df_read['error_num'].mean()) + '\\t' + type_index[typ -1], file = f1)\n",
    "            print(time_index[counter-1] + '\\t' + str(df_scrb['error_num'].mean()) + '\\t' + type_index[typ -1], file = f2)\n",
    "            counter = counter + 1\n",
    "    for ff in [f1, f2]:\n",
    "        ff.close()\n",
    "    print(\"Read/scrubbing error analysis done!\")\n",
    "\n",
    "def soft_hard_analysis(df_res):   ### Finding 11\n",
    "    time_index = [\"1m\",\"5m\",\"10m\",\"20m\",\"30m\",\"1h\",\"1d\",\"7d\",\"30d\"]\n",
    "    df_final = df_res.sort_values(by=['sid','memoryid','rankid','bankid','row','col','error_time'])\n",
    "    df_first = df_final.drop_duplicates(['sid','memoryid','rankid','bankid','row','col'],keep='first').rename(columns={'time_diff':'first_time'})\n",
    "    df_last = df_final.drop_duplicates(['sid','memoryid','rankid','bankid','row','col'],keep='last').rename(columns={'time_diff':'last_time'})\n",
    "    df_cell = df_first.merge(df_last, on = ['sid','memoryid','rankid','bankid','row','col'], how='inner')\n",
    "    df_cell['time_res'] = (df_cell['first_time'] - df_cell['last_time']) / 3600 / 24\n",
    "    df_cell['is_hard'] = df_cell['time_res'].apply(lambda x: 1 if np.round(x) >= 1 else 0)  ## if 0 soft error, else hard errors\n",
    "    df_final = df_final.merge(df_cell.loc[:,['sid','memoryid','rankid','bankid','row','col','is_hard']], on = ['sid','memoryid','rankid','bankid','row','col'], how='inner')\n",
    "    type_index = [\"ue\",\"ce\",\"misc\"]\n",
    "    f1 = open(\"./result/hard_error_mean.txt\",\"w\")\n",
    "    f2 = open(\"./result/soft_error_mean.txt\",\"w\")\n",
    "    for ff in [f1, f2]:\n",
    "        print('time\\tvalue\\tfailure_type',file=ff)\n",
    "    for typ in [1,2,3]:\n",
    "        counter = 1\n",
    "        df_type = df_final[df_final['failure_type'] == typ]\n",
    "        for tm in [pd.Timedelta(minutes=1),pd.Timedelta(minutes=5),pd.Timedelta(minutes=10),pd.Timedelta(minutes=20),pd.Timedelta(minutes=30),pd.Timedelta(hours=1),pd.Timedelta(days=1),pd.Timedelta(weeks=1),pd.Timedelta(days=30)]:\n",
    "            df_tm = df_type[df_type['time_diff'] > tm.total_seconds()]\n",
    "            df_hard_error = df_tm[df_tm['is_hard'] == 1].groupby(['sid'])['memoryid'].count().reset_index(name='error_num')\n",
    "            df_soft_error = df_tm[df_tm['is_hard'] == 0].groupby(['sid'])['memoryid'].count().reset_index(name='error_num')\n",
    "            print(time_index[counter-1] + '\\t' + str(df_hard_error['error_num'].mean()) + '\\t' + type_index[typ -1], file = f1)\n",
    "            print(time_index[counter-1] + '\\t' + str(df_soft_error['error_num'].mean()) + '\\t' + type_index[typ -1], file = f2)\n",
    "            counter = counter + 1\n",
    "    for ff in [f1, f2]:\n",
    "        ff.close()\n",
    "    print(\"hard/soft error analysis done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc25a625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read/scrubbing error analysis done!\n",
      "hard/soft error analysis done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "read_scrubbing_analysis(df_tickets_log)  ## Finding 9\n",
    "\n",
    "soft_hard_analysis(df_tickets_log)  ## Finding 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e560e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data\n",
    "df_mcelog = pd.read_csv('mcelog.csv')\n",
    "df_mcelog = df_mcelog.sample(1000)\n",
    "df_inventory = pd.read_csv( 'inventory.csv')\n",
    "df_inventory = df_inventory.sample(1000)\n",
    "df_tickets = pd.read_csv( 'trouble_tickets.csv')\n",
    "df_tickets = df_tickets.sample(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4260f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcelog.to_csv(\"mcelog_1k.csv\")\n",
    "df_inventory.to_csv(\"inventory_1k.csv\")\n",
    "df_tickets.to_csv(\"trouble_tickets_1k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b56cb542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 24841252 to 48213797\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   sid         1000 non-null   object\n",
      " 1   memoryid    1000 non-null   int64 \n",
      " 2   rankid      1000 non-null   int64 \n",
      " 3   bankid      1000 non-null   int64 \n",
      " 4   row         1000 non-null   int64 \n",
      " 5   col         1000 non-null   int64 \n",
      " 6   error_type  1000 non-null   int64 \n",
      " 7   error_time  1000 non-null   object\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 70.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 5632 to 16339\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   sid                  1000 non-null   object\n",
      " 1   server_manufacturer  1000 non-null   object\n",
      " 2   DRAM_model           1000 non-null   object\n",
      " 3   DIMM_number          1000 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 39.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 1087 to 1946\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   sid           1000 non-null   object\n",
      " 1   failure_type  1000 non-null   int64 \n",
      " 2   failed_time   1000 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 31.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_mcelog.info())\n",
    "print(df_inventory.info())\n",
    "print(df_tickets.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84153fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   sid  memoryid  rankid  bankid     row   col  error_type  \\\n",
      "24841252  Server_22773         0       1       4      69     0           2   \n",
      "1165557    Server_2119         4       0       9  103041   400           1   \n",
      "5807423    Server_7477        15       1       2  123742     0           2   \n",
      "30701315  Server_26851         9       1       5   17887  1016           2   \n",
      "62181207  Server_28769        12       0       0      13   600           1   \n",
      "\n",
      "                   error_time  \n",
      "24841252  0001-04-18 00:34:57  \n",
      "1165557   0001-08-19 05:38:49  \n",
      "5807423   0001-05-24 22:14:03  \n",
      "30701315  0001-03-06 10:00:21  \n",
      "62181207  0001-06-26 19:58:38  \n",
      "                sid server_manufacturer DRAM_model  DIMM_number\n",
      "5632    Server_5633                  M2         B1           24\n",
      "58        Server_59                  M2         C1           24\n",
      "12986  Server_12987                  M1         A1           24\n",
      "15086  Server_15087                  M1         C1           12\n",
      "14193  Server_14194                  M1         B1           12\n",
      "               sid  failure_type          failed_time\n",
      "1087  Server_26872             2  0001-07-01 07:04:49\n",
      "18    Server_10514             2  0001-01-22 16:59:49\n",
      "256   Server_13770             2  0001-05-16 21:58:14\n",
      "1306  Server_29882             2  0001-06-19 15:17:44\n",
      "1151  Server_27768             3  0001-03-31 05:43:43\n"
     ]
    }
   ],
   "source": [
    "print(df_mcelog.head())\n",
    "print(df_inventory.head())\n",
    "print(df_tickets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'result/'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    prefix_dir = sys.argv[1]\n",
    "    try:\n",
    "        os.mkdir(\"result/\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4c467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
